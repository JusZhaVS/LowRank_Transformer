<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Research Blog Title – Your Name</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Short 1–2 sentence description of this post." />

  <!-- Optional: MathJax for LaTeX-style math -->
  <!--
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  -->

  <style>
    :root {
      --bg: #f5f7fb;
      --bg-card: #ffffff;
      --border-subtle: #e0e4ef;
      --accent: #2563eb;
      --accent-soft: #e0ecff;
      --text-main: #111827;
      --text-muted: #6b7280;
      --code-bg: #f3f4f6;
      --font-body: "Times New Roman", Times, serif;
      --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --radius-lg: 12px;
      --shadow-soft: 0 14px 25px rgba(15, 23, 42, 0.08);
      --max-width: 1120px;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: var(--font-body);
      background-color: var(--bg);
      color: var(--text-main);
      line-height: 1.6;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    /* Layout */
    .page {
      min-height: 100vh;
      display: flex;
      flex-direction: column;
    }

    header {
      border-bottom: 1px solid var(--border-subtle);
      background: rgba(255, 255, 255, 0.9);
      backdrop-filter: blur(10px);
      position: sticky;
      top: 0;
      z-index: 10;
    }

    .header-inner {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 12px 16px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 12px;
    }

    .site-title {
      font-size: 18px;
      font-weight: 600;
    }

    .site-subtitle {
      font-size: 13px;
      color: var(--text-muted);
    }

    .header-right {
      font-size: 13px;
      color: var(--text-muted);
    }

    main {
      flex: 1;
      max-width: var(--max-width);
      margin: 24px auto;
      padding: 0 16px 48px;
      display: grid;
      grid-template-columns: minmax(0, 3fr) minmax(200px, 1.1fr);
      gap: 24px;
    }

    /* Article / content card */
    .article-card {
      background: var(--bg-card);
      border-radius: var(--radius-lg);
      box-shadow: var(--shadow-soft);
      padding: 24px 24px 32px;
      border: 1px solid var(--border-subtle);
    }

    .article-meta {
      font-size: 13px;
      color: var(--text-muted);
      margin-bottom: 12px;
    }

    h1.article-title {
      font-size: 28px;
      margin: 0 0 8px;
      letter-spacing: -0.02em;
    }

    .article-authors {
      font-size: 14px;
      color: var(--text-muted);
      margin-bottom: 12px;
    }

    .article-authors a {
      font-weight: 500;
    }

    .article-tags {
      font-size: 12px;
      color: var(--text-muted);
    }

    .tag-pill {
      display: inline-block;
      padding: 2px 8px;
      border-radius: 999px;
      background: var(--accent-soft);
      color: var(--accent);
      font-size: 11px;
      margin-right: 6px;
    }

    hr {
      border: none;
      border-top: 1px solid var(--border-subtle);
      margin: 24px 0;
    }

    /* Sections */
    section {
      margin-bottom: 32px;
    }

    section h2 {
      font-size: 20px;
      margin-bottom: 8px;
      border-left: 3px solid var(--accent);
      padding-left: 8px;
    }

    section h3 {
      font-size: 16px;
      margin-top: 18px;
      margin-bottom: 6px;
    }

    p {
      margin: 8px 0;
    }

    .abstract {
      background: #f9fafb;
      border-radius: 8px;
      padding: 12px 14px;
      border: 1px solid var(--border-subtle);
      font-size: 14px;
    }

    .callout {
      border-radius: 8px;
      border: 1px solid var(--accent-soft);
      background: #f9fbff;
      padding: 12px 14px;
      font-size: 14px;
    }

    .callout strong {
      color: var(--accent);
    }

    /* Figures */
    figure {
      margin: 20px 0;
    }

    figure img {
      display: block;
      width: 100%;
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid var(--border-subtle);
    }

    figcaption {
      font-size: 13px;
      color: var(--text-muted);
      margin-top: 6px;
      text-align: center;
    }

    /* Code / pre */
    pre {
      background: var(--code-bg);
      border-radius: 8px;
      padding: 10px 12px;
      overflow: auto;
      font-size: 13px;
      font-family: var(--font-mono);
      border: 1px solid var(--border-subtle);
    }

    code {
      font-family: var(--font-mono);
      font-size: 13px;
      background: var(--code-bg);
      padding: 2px 4px;
      border-radius: 4px;
    }

    blockquote {
      border-left: 3px solid var(--accent-soft);
      margin: 12px 0;
      padding-left: 12px;
      color: var(--text-muted);
      font-size: 14px;
      font-style: italic;
    }

    /* TOC / sidebar */
    aside {
      position: relative;
    }

    .toc {
      position: sticky;
      top: 84px;
      background: rgba(255, 255, 255, 0.96);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border-subtle);
      box-shadow: var(--shadow-soft);
      padding: 16px 16px 18px;
      max-height: calc(100vh - 120px);
      overflow-y: auto;
      font-size: 13px;
    }

    .toc-title {
      font-size: 14px;
      font-weight: 600;
      margin-bottom: 8px;
    }

    .toc small {
      color: var(--text-muted);
      font-size: 11px;
    }

    .toc ul {
      list-style: none;
      margin: 8px 0 0;
      padding: 0;
    }

    .toc li {
      margin: 3px 0;
    }

    .toc a {
      display: inline-block;
      padding: 2px 0;
    }

    .toc .toc-sub {
      padding-left: 12px;
      font-size: 12px;
    }

    /* Footer */
    footer {
      border-top: 1px solid var(--border-subtle);
      font-size: 12px;
      color: var(--text-muted);
      padding: 12px 16px 16px;
      text-align: center;
    }

    /* Responsive */
    @media (max-width: 900px) {
      main {
        grid-template-columns: minmax(0, 1fr);
      }

      aside {
        order: -1;
      }

      .toc {
        position: static;
        max-height: none;
        margin-bottom: 12px;
      }
    }

    @media (max-width: 600px) {
      .article-card {
        padding: 18px 16px 24px;
      }

      h1.article-title {
        font-size: 24px;
      }
    }
  </style>
</head>
<body>
<div class="page">
  <main>
    <!-- Main article -->
    <article class="article-card">
      <div class="article-meta">Final · Last updated: 2025-12-09</div>
      <h1 class="article-title">Factored Attention Projections (FAP) for Parameter-Efficient Transformers</h1>

      <div class="article-authors">
        <a href="https://github.com/HatedFate">Xin Qi Liu<sup>†</sup></a>,
        <a>Justin Zhang</a>,
        <a>Nico</a>
      </div>
      <hr />

      <!-- Abstract -->
      <section id="abstract">
        <h2>Abstract</h2>
        <div class="abstract">
          <p>
            Transformer architectures are the backbone of modern Large Language Models (LLMs) and are
            increasingly used across vision, audio, and multimodal tasks, but their large parameter counts
            make deployment on consumer hardware challenging. Existing compression techniques such as quantization
            effectively reduce memory footprint by lowering weight precision, yet they do not reduce the number
            of parameters or structurally compress the attention mechanism itself. We introduce <b>Factored Attention
            Projections (FAP)</b>, a method that replaces the standard full-rank query, key, and value projection
            matrices with low-rank factorizations, thereby reducing the parameter count of the attention blocks while
            leaving the softmax attention operation unchanged. FAP is complementary to quantization, as it targets
            model structure rather than numerical precision. In our experiments, FAP achieves performance comparable
            to models with full-rank attention projections while using fewer attention parameters, suggesting that
            these projections are amenable to low-rank parameterization without substantial loss in quality.
          </p>
        </div>
      </section>

      <!-- Introduction -->
      <section id="introduction">
        <h2>1. Introduction</h2>

        <p>
          Modern deep learning systems for language, vision, and even video are typically built by stacking many
          Transformer layers, following the architecture introduced by Vaswani et al [1]. This design underlies large 
          language models such as GPT-3, an autoregressive Transformer with 175 billion parameters, as reported in the 
          original NeurIPS paper “Language Models are Few-Shot Learners.” Even “smaller” open models remain at 
          multi-billion–parameter scale. For example, Mistral 7B is a 7-billion-parameter [2] Transformer, Llama 2 
          provides models ranging from 7 billion to 70 billion parameters, and DeepSeek LLM includes variants up to 
          67 billion parameters [3]. Training and serving such models require substantial computation and energy, with
          empirical studies showing that state-of-the-art NLP and large neural networks incur significant financial 
          cost and carbon emissions [4]. Because these multi-billion-parameter Transformers strain the memory, latency,
          and power budgets of edge hardware, a large body of work now focuses on efficient Transformer variants,
          model compression, and on-device inference to make deployment on resource-constrained devices more 
          feasible [5].
        </p>
        <p>
          These challenges have motivated extensive work on optimizing Transformer models to reduce model size and 
          inference latency. Over the past 3 - 5 years, many techniques have been proposed and adopted, but most notably,
          quantization has emerged as one of the most practical and widely used approaches for compressing models and 
          accelerating inference, especially on hardware with efficient low-precision support. The core idea of 
          quantization is to represent model parameters (and sometimes activations) with lower numerical precision. 
          Instead of storing all weights in 32-bit floating-point format, recent methods use 8-bit and even 4-bit
          representations. In the ideal case, moving from 32-bit to 8-bit parameters reduces parameter storage by 75%,
          and moving to 4-bit reduces it by 87.5%, assuming all eligible weights are quantized. This substantially 
          lowers the memory footprint and, when low-precision operations are well supported by the hardware, can also 
          significantly reduce inference time.
        </p>
      <section id="motivation">
        <h2>2. Motivation</h2>
        <p>
          This article investigates an alternative axis of model compression that targets the attention mechanism itself,
          by factorizing the attention projection matrices into a product of two low-rank matrices. Most prior work on
          compressing Transformers, including pruning and the quantization techniques discussed above, has focused
          primarily on reducing the size of the Feed Forward Networks (FFNs) which is often the largest component of 
          modern day transformers. In contrast, applying aggressive pruning or low-precision quantization to multi-head 
          attention (MHA) can directly affect the model's outputs [6, 7]. In practice, attention projections are often more 
          sensitive to changes in structure or numerical precision, so naively pruning or quantizing these components 
          can lead to a significant drop in quality [7]. Our proposed FAP method reduces the number of trainable parameters
          in the attention layers by replacing full-rank projections with low-rank factorizations, with the goal of 
          reducing parameters while maintaining model quality in our experiments.
        </p>
      </section>

      <!-- Background -->
      <section id="Background">
        <h2> 3. Background </h2>

        <h3 id="quantization">3.1 Quantization</h3>
        <p>
          Quantization has become one of the most widely used methods for model compression because 
          it can substantially reduce memory and compute while often preserving model quality. Many practical 
          schemes primarily target lowering the precision of model parameters (particularly with FFNs), with 
          activations initially kept in higher precision. Most recent works on quantization methods can be categorized
          into two groups: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ) [7]. In QAT,
          the model is trained or fine-tuned while using low-precision parameters (e.g. 8-bits floating point),
          so that the optimization can compensate for quantization error. LLM-QAT is a representative example 
          that quantizes weights, activations, and even the KV cache, and then performs data-free distillation
          from the original model to counteract model degradation [8]. However, QAT typically requires substantial 
          compute and memory, which makes it difficult to apply to very large models in practice.
        </p>

        <p>
          On the other hand, PTQ starts from a fully trained model and quantizes it after training without fine-tuning 
          at all. Typical PTQ pipelines gather a small calibration set, estimate statistics of weights and activations, 
          and then choose quantization scales and zero-points (e.g. channel-wise or group-wise) to minimize some error 
          metric. For LLMs, many PTQ methods focus on weight-only quantization to 8-bits, 4-bits, or even 3-bits and can 
          often be applied with no additional training. Examples include ZeroQuant, which introduces efficient PTQ for
          large Transformer models [9]. Additionally, GPTQ and other related methods use approximate second-order 
          information to achieve accurate 3-bit to 4-bit weight-only quantization [10]. SmoothQuant, which enables 8-bit
          weights and activations for many LLM architectures via an activation-smoothing transformation [11]. Compared
          to QAT, PTQ is often easier and cheaper to deploy and is therefore the most common choice in real systems, 
          although its accuracy can degrade at extremely low bit-widths or under strong hardware constraints. 
        </p>

        <h3 id="low-ranks">3.2 Low-Rank Approximation</h3>
        <p>
          Training pipelines of Modern LLMs include large-scale pretraining on general-domain data before undergoing 
          adaptation to a particular task. However, recent models such as GPT, Gemini, and Claude use very large 
          transformer architectures, often with tens or hundreds of billions of parameters, and even consumer or open 
          models such as Mistral and LLaMA can have tens of billions of parameters. Thus, full fine-tuning (i.e., 
          updating every single weight within the model) becomes extremely expensive and often impractical. 
        </p>

        <p>
          To address this issue, researchers at Microsoft introduced LoRA (Low-Rank Adaptation of Large Language Models) 
          [12]. LoRA adds a small trainable module that represents the weight update as the product of two low-rank
           matrices, while the original model weights are kept frozen. The key assumption is that, for large 
           overparameterized models, the weight update during fine-tuning lies in a much lower-dimensional subspace. 
           Thus, the actual gradient update becomes the following:
        </p>

        <div style="position: relative; margin: 1em 0; font-family: 'Times New Roman', Times, serif; font-size: 1.5em;">
          <p style="text-align: center; font-style: italic; margin: 0;">
            h = W<sub><span style="font-style: normal;">0</span></sub> + <span style="font-style: normal;">&Delta;</span>W
          </p>
          <span style="position: absolute; right: 0; top: 0; font-style: normal;">
            (1)
          </span>
        </div>

        <p>
          Here, <i>W<sub><span style="font-style: normal;">0</span></sub></i> denotes the weight of the base model, and the final updated matrix is <i>h</i>. 
          Additionally, we have the update matrix denoted as <span style="font-style: normal;">&Delta;</span><i>W</i>. In particular,
          we set <span style="font-style: normal;">&Delta;</span><i>W</i> = <i>A</i><i>B</i>, where <i>A</i> &in;
          &#8477;<sup><i>d</i><sub><i>out</i></sub> &times; <i>r</i></sup> and <i>B</i> &in; &#8477;<sup><i>r</i>
          &times; <i>d</i><sub><i>in</i></sub></sup>. These LoRA layers are inserted across the model in every transformer 
          block to approximate the necessary gradient updates for fine-tuning without having to update the parameters of the
          base model. 
        </p>

        <p>
          With the introduction of LoRA, it has inspired several follow-up methods such as QLoRA, which combines 
          low-rank adaptation with low-precision quantization [13]. QLoRA further decreases the memory footprint and 
          inference time by quantizing the base model weights to 4-bit precision while keeping a small set of LoRA 
          adapter weights in higher precision (i.e. 16-bit floating point) for training. In this setup, the quantized 
          base model stays fixed, and only the higher-precision LoRA parameters are updated, allowing the fine-tuned 
          model to approach the performance of a standard 16-bit or 32-bit model while using much less memory during 
          both training and inference.
        </p>

      </section>

      <!-- Methods / Main Idea -->
      <section id="methods">
        <h2>4. Methodology </h2>
        <p>
          In this section we describe <b>FAP</b>, a simple architectural modification that 
          replaces the standard full-rank query, key, and value projection matrices with low-rank
          factorizations.
        </p>

        <h3 id="low-projections">4.1 Low-Rank Projections</h3>

        <p>
          In standard transformers, the queries, keys, and values projection are defined as follows:
        </p>
          <div style="position: relative; margin: 1em 0; font-family: 'Times New Roman', Times, serif; font-size: 1.5em;">
            <p style="text-align: center; font-style: italic; margin: 0;">
              Q = X W<sub>Q</sub><br>
              K = X W<sub>K</sub><br>
              V = X W<sub>V</sub>
            </p>
            <span style="position: absolute; right: 0; top: 0; font-style: normal;">
              (2)
            </span>
          </div>

        <p>
          Generally, the projection matrices <i>W<sub>Q</sub></i>, <i>W<sub>K</sub></i>, and <i>W<sub>V</sub></i> are in 
          &#8477;<sup><i>d</i><sub>model</sub> &times; <i>d<sub>k</sub></i></sup>. 
          The core assumption of FAP is that all information required by the attention mechanism lies in a low-rank subspace. 
          We therefore introduce a hyperparameter <i>r</i> (the rank), such that:
        </p>

        <div>
          <p style="text-align: center; font-style: italic; margin: 1.0em 1; font-size: 1.5em;">
            W<sub>Q</sub> = A<sub>Q</sub>B<sub>Q</sub> <br>
            W<sub>K</sub> = A<sub>K</sub>B<sub>K</sub> <br>
            W<sub>V</sub> = A<sub>V</sub>B<sub>V</sub>
          </p>
            <span style="position: absolute; right: 0; top: 0; font-style: normal;">
              (3)
            </span>
        </div>
        <p>
          where <i>A</i> &in; &#8477;<sup><i>d</i><sub>model</sub>&times;<i>r</i></sup> and 
          <i>B</i> &in; &#8477;<sup><i>r</i>&times;<i>d</i><sub>k</sub></sup>. 
          Consequently, this low-rank assumption imposes an inherent limitation on the choice of <i>r</i>, 
          which must satisfy <i>r</i> &le; min(<i>d</i><sub>model</sub>, <i>d</i><sub>k</sub>). If <i>r</i> is greater than
          <i>d</i><sub>model</sub> or  <i>d</i><sub>k</sub>, then the model is overdetermined, and it will completely delete
          the purpose of using FAP to reduce trainable parameters. A snippet of the code is shown below:
        </p>

<pre><code>
  ...

  self.A = nn.Linear(in_features=in_features, out_features=rank, bias=False)
  self.B = nn.Linear(in_features=rank, out_features=out_features, bias=bias)

  # Initialize like LoRA (A random, B near-zero)
  nn.init.kaiming_uniform_(self.A.weight, a=sqrt(5))
  nn.init.zeros_(self.B.weight)

  if bias and self.B.bias is not None:
      nn.init.zeros_(self.B.bias)
  
  ...
</code></pre>

        <p>
          To see the parameter reduction explicitly, note that a standard projection matrix 
          <i>W</i> &in; &#8477;<sup><i>d</i><sub>model</sub>&times;<i>d</i><sub><i>k</i></sub></sup> contains
        </p>

        <p style="text-align: center; margin: 0.5em 0; font-size: 1.5em;">
          <i>W</i><sub>full</sub> = <i>d</i><sub>model</sub> &times; <i>d</i><sub><i>k</i></sub>
        </p>

        <p>
          parameters. Under the low-rank factorization <i>W</i> = <i>A</i><i>B</i>, the total number of parameters becomes
        </p>

        <p style="text-align: center; margin: 0.5em 0; font-size: 1.5em;">
          <i>W</i><sub>low-rank</sub> = <i>d</i><sub>model</sub> &times; <i>r + r </i>&times; <i>d<sub>k</sub> = r </i>(<i>d</i><sub>model</sub> + <i>d</i><sub><i>k</i></sub>).
        </p>

        <p>
          Thus, the number of parameters saved is
        </p>

        <p style="text-align: center; margin: 0.5em 0; font-size: 1.5em;">
          <i>W</i><sub>full</sub> &minus; <i>W</i><sub>low-rank</sub> =
          <i>d</i><sub>model</sub> <i>d</i><sub>k</sub> &minus;
          <i>r</i> (<i>d</i><sub>model</sub> + <i>d</i><sub>k</sub>).
        </p>


        <p>
          In the common case where <i>d</i><sub>model</sub> &approx; <i><sub>k</sub> = d</i> and <i>r</i> &laquo; <i>d</i>, 
          the full matrix scales as &Theta;(<i>d</i><sup>2</sup>), whereas the low-rank factorization scales as &Theta;(<i>rd</i>), 
          effectively reducing a quadratic parameter growth to (approximately) linear in the model dimension.
        </p>


        <h3 id="models">4.2 Models</h3>
        <p>
          Due to resource constraints, we were not able to recreate a low-rank version of GPT-3 and
          instead opted to create a smaller causal language model. Additionally, we evaluated the
          effectiveness of low-rank projections not only on next-token prediction, but also on
          image classification by redefining Vision Transformers (ViT) using low-rank projections. 
          For the causal language model, we used a small transformer model. The hyperparameters of the 
          models are below:
        </p>

        <table
          style="
            border-collapse: collapse;
            margin: 1em auto;
            font-family: 'Times New Roman', Times, serif;
            font-size: 0.95em;
            min-width: 320px;
            box-shadow: 0 0 4px rgba(0,0,0,0.2);
          "
        >
          <caption style="caption-side: bottom; padding-top: 0.5em; font-style: italic;">
            Table 1: Language Model Hyperparameters
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2;">
              <th
                style="
                  border: 1px solid #000;
                  padding: 6px 10px;
                  text-align: left;
                "
              >
                Hyperparameter
              </th>
              <th
                style="
                  border: 1px solid #000;
                  padding: 6px 10px;
                  text-align: center;
                "
              >
                Value
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;">Vocabulary Size</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">50257</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;"># of layers</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">32</td>
            </tr>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;"># of Attentions Per Layer </td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">64</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;">Hidden size (<i>d</i><sub>model</sub>)</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">786</td>
            </tr>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;">Feedforward size</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">3072</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;">Max Positional Embedding</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">1024</td>
            </tr>
          </tbody>
        </table>

        <p>
          The hyperparameters of the ViT are below:
        </p>


                <table
          style="
            border-collapse: collapse;
            margin: 1em auto;
            font-family: 'Times New Roman', Times, serif;
            font-size: 0.95em;
            min-width: 320px;
            box-shadow: 0 0 4px rgba(0,0,0,0.2);
          "
        >
          <caption style="caption-side: bottom; padding-top: 0.5em; font-style: italic;">
            Table 2: ViT Hyperparameters
          </caption>
          <thead>
            <tr style="background-color: #f2f2f2;">
              <th
                style="
                  border: 1px solid #000;
                  padding: 6px 10px;
                  text-align: left;
                "
              >
                Hyperparameter
              </th>
              <th
                style="
                  border: 1px solid #000;
                  padding: 6px 10px;
                  text-align: center;
                "
              >
                Value
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;">In Channels</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">3</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;">Out Channels</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">10</td>
            </tr>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;">Image Size</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">32</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;">Hidden size (<i>d</i><sub>model</sub>)</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">64</td>
            </tr>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;">Feedforward size</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">128</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;"># of Hidden Layers</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">6</td>
            </tr>
            <tr>
              <td style="border: 1px solid #000; padding: 6px 10px;"># of Attentions per Layer</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">3</td>
            </tr>
            <tr style="background-color: #fafafa;">
              <td style="border: 1px solid #000; padding: 6px 10px;">Patch Size</td>
              <td style="border: 1px solid #000; padding: 6px 10px; text-align: center;">4</td>
            </tr>
          </tbody>
        </table>

      In both models, the FFN modules use the Gaussian Error Linear Unit (GeLU) activation. We also employ
      RMSNorm to improve training stability. In the ViT, we use learned patch embeddings with a fixed 
      patch size, primarily to demonstrate the proof of concept for our approach, though Rotary Position 
      Embeddings (RoPE) could be incorporated into both models. Lastly, the temperature when prompting
      the causual language is 0.7.

      <h3 id="core-idea">4.2 Training Setup</h3>
      <p>
        For both models, we trained with a batch size of 16, a learning rate of 
        5 &times; 10<sup>&minus;4</sup>, and a weight decay of 0.01. Training was run for 3 epochs with 
        1000 warmup steps and a maximum gradient norm of 1.0 for gradient clipping. We also performed 
        periodic checkpointing and evaluation, saving the model every 1000 steps and evaluating every 500 steps.
      </p>

      <h3 id="core-idea">4.3 Dataset</h3>
      <p>
        We used the TinyStories dataset for training the causal language model, primarily due to resource constraints,
        and evaluated the ViT on the CIFAR-10 dataset.
      </p>

      </section>

      <!-- Experiments -->
      <section id="experiments">
        <h2>5. Evaluation</h2>

        <p>
          For all evaluations, we use the following ranks: 4, 8, 16, 20, 32, and 40. This range covers a 
          meaningful portion of the rank spectrum and allows us to analyze both where the low-rank model 
          begins to deteriorate and where it remains relatively stable compared to the baseline full-rank model.
        </p>

        <h3 id="casual-eval">5.2 Causal Language Model Evaluation Metrics</h3>
          <p>
            Our primary quantitative metric for the causal language model is next-token prediction performance on the TinyStories dataset,
            evaluated in terms of perplexity. Our objective is to minimize the perplexity of the model. In addition, we perform a qualitative
            evaluation by prompting the model and comparing its generations to those of a full-rank baseline model without any low-rank
            approximation.
          </p>

        <h3 id="vit-eval">5.3 ViT Evaluation Metrics</h3>
          <p>
            For evaluating our ViT, we trained on the CIFAR-10 dataset and measured performance using standard top-1
            classification accuracy on a held-out validation set. We compared low-rank variants against a full-rank
            baseline model trained with identical hyperparameters to isolate the effect of the low-rank approximation.
            In addition, we inspected the attention maps across layers and heads to visualize how the attention patterns
            change under low-rank factorization and to verify that the model still focuses on semantically meaningful
            regions of the input images.
          </p>

      </section>

      <!-- Results -->
      <section id="Results">
        <h2>6. Results</h2>
        <h3>6.1 Perplexity Analysis</h3>
        <div style="text-align: center; margin: 1em 0;">
          <img src="results/perplexity-anal.png"
              alt="Perplexity analysis for LowRank Transformer"
              style="max-width: 100%; height: auto;">
        </div>

        <p>
          The perplexity of the language model remains relatively stable for ranks greater than 8, 
          but increases significantly when the rank is reduced to 4. 
        </p>

        <h3>6.4 Classification Accuracy</h3>

        <h3>6.5 Counting Trainable Parameters</h3>
        <div style="text-align: center; margin: 1em 0;">
          <img src="results/parameters-anal.png"
              alt="Perplexity analysis for LowRank Transformer"
              style="max-width: 100%; height: auto;">
        </div>

        <p>
          As predicted by the asymptotic analysis of parameter savings, the 
          number of parameters varies linearly with the rank.
        </p>

      </section>

      <section id="Discussion">
        <h2>7. Discussion </h2>
        <p>
          The results from applying our low-rank projection to the transformer suggest that an effective model 
          can indeed exist in a low-rank subspace. In other words, our experiments show that, with a sufficiently 
          large rank, low-rank approximations can achieve performance comparable to a full-rank model. Overall, ranks
           between 8 and 40 appear sufficient both to fit the training data and to generalize well to the broader data
          distribution.
        </p>

        <p>
          From the causal language model perspective, we observe that perplexity increases dramatically at extremely 
          low ranks such as 4, but for ranks above 8 it remains within roughly 2% of the baseline. This sharp degradation 
          at very small ranks is likely due to reduced attention diversity caused by the low rank approximation, which 
          limits the model’s ability to attend to relevant tokens (visualization of the attention in ViT would explain 
          this effect). Qualitatively, models with ranks between 8 and 40 produce outputs with expressiveness comparable
          to the full rank model. However, across all configurations, including the baseline, the model fails to answer 
          questions outside its training distribution. This limitation is primarily due to the small model size and the  
          restricted training data, rather than the use of low rank projections.
        </p>

        <p>
          For the ViT, we also observe very similar results for ranks between 8 and 40, achieving validation accuracy
          within 2% to 5% of the baseline. The effect of the low rank approximation on the attention mechanism is clear 
          in the visualizations: the attention becomes more dispersed and less sharply focused than in the baseline model. 
          Despite this, the model still reaches comparable accuracy, possibly because the essential global structure of the
          images is preserved in a lower dimensional subspace, allowing the network to rely on coarse but still informative
          patterns rather than very sharp attention peaks.
        </p>
      
        <h3>7.1 Is this worth it?</h3>

        <p>
          Whether this method is worthwhile to use in modern transformer based models depends strongly on the deployment
          setting. Large models such as ChatGPT, Gemini, and Claude are typically served on powerful infrastructure with 
          ample memory and compute, where there is less pressure to rely on bottleneck techniques like pruning, quantization,
          or low rank approximation of projections. In contrast, our approach is most relevant for resource constrained 
          environments such as laptops, mobile devices, and other on-device inference settings, where even modest savings 
          in parameters and memory can be critical. In short, our FAP method is geared toward on-device inference, enabling
          models to use less space and compute as they continue to grow in size and complexity. Even so, approximating 
          the projection matrices alone will save about 10 - 15% of the total model size. For example, in the case of our
          causal language model, rank 8 reduces the amount of parameters by about 100 million, which is equivalent to 
          about 180 MB, whereas the full model is 1.2 GB.
        </p>
      </section>

      <section id="Limitations">
        <h2>8. Limitations</h2>
        <p>
          While our method does save a respectable number of parameters, it does not necessarily result in a faster model.
          Many hardware efficient transformer algorithms, such as Flash Attention [14] and Xformers [15], are optimized for 
          dense rectangular projection matrices, so reducing the parameter count via low rank approximation does not automatically 
          translate into lower inference latency on such systems. Thus, in this work, we do not focus on the Floating-Point 
          Operations Per Second (FLOPs) as part of our analysis since this is a completely different axis optimization, and 
          it is not a great one at that either largely because it is hardware dependent.
        </p>
        <p>
          Another limitation of our study is that we evaluate the architecture on relatively easy tasks, primarily due 
          to resource constraints. A more thorough analysis would require harder benchmarks, such as datasets that depend
          strongly on long range context (for example, some IMO or IOI style problems) or more challenging vision tasks
          like ImageNet scale classification or image generation. Such experiments are needed to more convincingly 
          demonstrate that low rank approximations can be applied reliably to tougher, real world workloads.
        </p>
      </section>

      <!-- References -->
      <section id="references">
        <h2>References</h2>
        <p style="font-size: 24px; color: var(--text-muted);">
        </p>
        <ul style="list-style-type: none; padding-left: 0; margin-left: 0.5em;">
          <li>1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2023). Attention Is All You Need. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
          <li>2. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li>
          <li>3. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., &amp; Sayed, W. E. (2023). Mistral 7B. <a href="https://arxiv.org/abs/2310.06825">https://arxiv.org/abs/2310.06825</a></li>
          <li>4. Strubell, E., Ganesh, A., &amp; McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. <a href="https://arxiv.org/abs/1906.02243">https://arxiv.org/abs/1906.02243</a></li>
          <li>5. Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2022). Efficient Transformers: A Survey. ACM Comput. Surv., 55(6). <a href="https://doi.org/10.1145/3530811">https://doi.org/10.1145/3530811</a></li>
          <li>6. Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., &amp; Chen, J. (2025). SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference. <a href="https://arxiv.org/abs/2502.18137">https://arxiv.org/abs/2502.18137</a></li>
          <li>7. Zhu, X., Li, J., Liu, Y., Ma, C., &amp; Wang, W. (2024). A Survey on Model Compression for Large Language Models. <a href="https://arxiv.org/abs/2308.07633">https://arxiv.org/abs/2308.07633</a></li>
          <li>8. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., &amp; Chandra, V. (2023). LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. <a href="https://arxiv.org/abs/2305.17888">https://arxiv.org/abs/2305.17888</a></li>
          <li>9. Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., &amp; He, Y. (2022). ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. <a href="https://arxiv.org/abs/2206.01861">https://arxiv.org/abs/2206.01861</a></li>
          <li>10. Frantar, E., Ashkboos, S., Hoefler, T., &amp; Alistarh, D. (2023). GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. <a href="https://arxiv.org/abs/2210.17323">https://arxiv.org/abs/2210.17323</a></li>
          <li>11. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., &amp; Han, S. (2024). SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. <a href="https://arxiv.org/abs/2211.10438">https://arxiv.org/abs/2211.10438</a></li>
          <li>12. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></li>
          <li>13. Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <a href="https://arxiv.org/abs/2305.14313">https://arxiv.org/abs/2305.14313</a></li>
          <li>14. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
          <li>15. Lefaudeux, B., Massa, F., Liskovich, D., Xiong, W., Caggiano, V., Naren, S., Xu, M., Hu, J., Tintore, M., Zhang, S., Labatut, P., Haziza, D., Wehrstedt, L., Reizenstein, J., & Sizov, G. (2022). xFormers: A modular and hackable Transformer modelling library. <a href="https://github.com/facebookresearch/xformers">https://github.com/facebookresearch/xformers</a></li>
        </ul>
      </section>
    </article>

    <!-- Sidebar / TOC -->
<!-- Sidebar / TOC -->
<aside>
  <nav class="toc">
    <div class="toc-title">On this page</div>
    <small>Jump to any section</small>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">1. Introduction</a></li>
      <li><a href="#motivation">2. Motivation</a></li>

      <li><a href="#Background">3. Background</a></li>

        <ul class="toc-sub">
          <li><a href="#quantization">3.1 Quantization</a></li>
          <li><a href="#low-ranks">3.2 Low-Rank Approximation</a></li>
        </ul>

      <li><a href="#methods">4. Methodology</a></li>
        <ul class="toc-sub">
          <li><a href="#low-projections">4.1 Low-Rank Projections</a></li>
          <li><a href="#models">4.2 Models</a></li>
        </ul>
      <li>
        <a href="#experiments">5. Evaluation</a>
        <ul class="toc-sub">
          <li><a href="#casual-eval">5.2 Causal LM Evaluation Metrics</a></li>
          <li><a href="#vit-eval">5.3 ViT Evaluation Metrics</a></li>
        </ul>
      </li>

      <li><a href="#Results">6. Results</a></li>
      <li><a href="#Discussion">7. Discussion</a></li>
      <li><a href="#Limitations">8. Limitations</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>
</aside>

  </main>

  <footer>
    © 2025 Your Name · Built as a simple static HTML template.
  </footer>
</div>
</body>
</html>
