<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Research Blog Title – Your Name</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Short 1–2 sentence description of this post." />

  <!-- Optional: MathJax for LaTeX-style math -->
  <!--
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  -->

  <style>
    :root {
      --bg: #f5f7fb;
      --bg-card: #ffffff;
      --border-subtle: #e0e4ef;
      --accent: #2563eb;
      --accent-soft: #e0ecff;
      --text-main: #111827;
      --text-muted: #6b7280;
      --code-bg: #f3f4f6;
      --font-body: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      --radius-lg: 12px;
      --shadow-soft: 0 14px 25px rgba(15, 23, 42, 0.08);
      --max-width: 1120px;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: var(--font-body);
      background-color: var(--bg);
      color: var(--text-main);
      line-height: 1.6;
    }

    a {
      color: var(--accent);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    /* Layout */
    .page {
      min-height: 100vh;
      display: flex;
      flex-direction: column;
    }

    header {
      border-bottom: 1px solid var(--border-subtle);
      background: rgba(255, 255, 255, 0.9);
      backdrop-filter: blur(10px);
      position: sticky;
      top: 0;
      z-index: 10;
    }

    .header-inner {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 12px 16px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 12px;
    }

    .site-title {
      font-size: 18px;
      font-weight: 600;
    }

    .site-subtitle {
      font-size: 13px;
      color: var(--text-muted);
    }

    .header-right {
      font-size: 13px;
      color: var(--text-muted);
    }

    main {
      flex: 1;
      max-width: var(--max-width);
      margin: 24px auto;
      padding: 0 16px 48px;
      display: grid;
      grid-template-columns: minmax(0, 3fr) minmax(200px, 1.1fr);
      gap: 24px;
    }

    /* Article / content card */
    .article-card {
      background: var(--bg-card);
      border-radius: var(--radius-lg);
      box-shadow: var(--shadow-soft);
      padding: 24px 24px 32px;
      border: 1px solid var(--border-subtle);
    }

    .article-meta {
      font-size: 13px;
      color: var(--text-muted);
      margin-bottom: 12px;
    }

    h1.article-title {
      font-size: 28px;
      margin: 0 0 8px;
      letter-spacing: -0.02em;
    }

    .article-authors {
      font-size: 14px;
      color: var(--text-muted);
      margin-bottom: 12px;
    }

    .article-authors a {
      font-weight: 500;
    }

    .article-tags {
      font-size: 12px;
      color: var(--text-muted);
    }

    .tag-pill {
      display: inline-block;
      padding: 2px 8px;
      border-radius: 999px;
      background: var(--accent-soft);
      color: var(--accent);
      font-size: 11px;
      margin-right: 6px;
    }

    hr {
      border: none;
      border-top: 1px solid var(--border-subtle);
      margin: 24px 0;
    }

    /* Sections */
    section {
      margin-bottom: 32px;
    }

    section h2 {
      font-size: 20px;
      margin-bottom: 8px;
      border-left: 3px solid var(--accent);
      padding-left: 8px;
    }

    section h3 {
      font-size: 16px;
      margin-top: 18px;
      margin-bottom: 6px;
    }

    p {
      margin: 8px 0;
    }

    .abstract {
      background: #f9fafb;
      border-radius: 8px;
      padding: 12px 14px;
      border: 1px solid var(--border-subtle);
      font-size: 14px;
    }

    .callout {
      border-radius: 8px;
      border: 1px solid var(--accent-soft);
      background: #f9fbff;
      padding: 12px 14px;
      font-size: 14px;
    }

    .callout strong {
      color: var(--accent);
    }

    /* Figures */
    figure {
      margin: 20px 0;
    }

    figure img {
      display: block;
      width: 100%;
      max-width: 100%;
      border-radius: 10px;
      border: 1px solid var(--border-subtle);
    }

    figcaption {
      font-size: 13px;
      color: var(--text-muted);
      margin-top: 6px;
      text-align: center;
    }

    /* Code / pre */
    pre {
      background: var(--code-bg);
      border-radius: 8px;
      padding: 10px 12px;
      overflow: auto;
      font-size: 13px;
      font-family: var(--font-mono);
      border: 1px solid var(--border-subtle);
    }

    code {
      font-family: var(--font-mono);
      font-size: 13px;
      background: var(--code-bg);
      padding: 2px 4px;
      border-radius: 4px;
    }

    blockquote {
      border-left: 3px solid var(--accent-soft);
      margin: 12px 0;
      padding-left: 12px;
      color: var(--text-muted);
      font-size: 14px;
      font-style: italic;
    }

    /* TOC / sidebar */
    aside {
      position: relative;
    }

    .toc {
      position: sticky;
      top: 84px;
      background: rgba(255, 255, 255, 0.96);
      border-radius: var(--radius-lg);
      border: 1px solid var(--border-subtle);
      box-shadow: var(--shadow-soft);
      padding: 16px 16px 18px;
      max-height: calc(100vh - 120px);
      overflow-y: auto;
      font-size: 13px;
    }

    .toc-title {
      font-size: 14px;
      font-weight: 600;
      margin-bottom: 8px;
    }

    .toc small {
      color: var(--text-muted);
      font-size: 11px;
    }

    .toc ul {
      list-style: none;
      margin: 8px 0 0;
      padding: 0;
    }

    .toc li {
      margin: 3px 0;
    }

    .toc a {
      display: inline-block;
      padding: 2px 0;
    }

    .toc .toc-sub {
      padding-left: 12px;
      font-size: 12px;
    }

    /* Footer */
    footer {
      border-top: 1px solid var(--border-subtle);
      font-size: 12px;
      color: var(--text-muted);
      padding: 12px 16px 16px;
      text-align: center;
    }

    /* Responsive */
    @media (max-width: 900px) {
      main {
        grid-template-columns: minmax(0, 1fr);
      }

      aside {
        order: -1;
      }

      .toc {
        position: static;
        max-height: none;
        margin-bottom: 12px;
      }
    }

    @media (max-width: 600px) {
      .article-card {
        padding: 18px 16px 24px;
      }

      h1.article-title {
        font-size: 24px;
      }
    }
  </style>
</head>
<body>
<div class="page">
  <main>
    <!-- Main article -->
    <article class="article-card">
      <div class="article-meta">Final · Last updated: 2025-11-27</div>
      <h1 class="article-title">Factored Attention Projections (FAP) for Parameter-Efficient Transformers</h1>

      <div class="article-authors">
        <a href="https://github.com/HatedFate">Xin Qi Liu</a>,
        <a href="https://pornhub.com">Justin Zhang</a>,
        <a href="https://pornhub.com">Nico</a>
      </div>
      <hr />

      <!-- Abstract -->
      <section id="abstract">
        <h2>Abstract</h2>
        <div class="abstract">
          <p>
            Transformer architectures are the backbone of modern Large Language Models (LLMs) and are
            increasingly used across vision, audio, and multimodal tasks, but their large parameter counts
            make deployment on consumer hardware challenging. Existing compression techniques such as quantization
            effectively reduce memory footprint by lowering weight precision, yet they do not reduce the number
            of parameters or structurally compress the attention mechanism itself. We introduce <b>Factored Attention
            Projections (FAP)</b>, a method that replaces the standard full-rank query, key, and value projection
            matrices with low-rank factorizations, thereby reducing the parameter count of the attention blocks while
            leaving the softmax attention operation unchanged. FAP is complementary to quantization, as it targets
            model structure rather than numerical precision. In our experiments, FAP achieves performance comparable
            to models with full-rank attention projections while using fewer attention parameters, suggesting that
            these projections are amenable to low-rank parameterization without substantial loss in quality.
          </p>
        </div>
      </section>

      <!-- Introduction -->
      <section id="introduction">
        <h2>1. Introduction</h2>

        <p>
          Modern deep learning systems for language, vision, and even video are typically built by stacking many
          Transformer layers, following the architecture introduced by Vaswani et al [1]. This design underlies large 
          language models such as GPT-3, an autoregressive Transformer with 175 billion parameters, as reported in the 
          original NeurIPS paper “Language Models are Few-Shot Learners.” Even “smaller” open models remain at 
          multi-billion–parameter scale. For example, Mistral 7B is a 7-billion-parameter [2] Transformer, Llama 2 
          provides models ranging from 7 billion to 70 billion parameters, and DeepSeek LLM includes variants up to 
          67 billion parameters [3]. Training and serving such models require substantial computation and energy, with
          empirical studies showing that state-of-the-art NLP and large neural networks incur significant financial 
          cost and carbon emissions [4]. Because these multi-billion-parameter Transformers strain the memory, latency,
          and power budgets of edge hardware, a large body of work now focuses on efficient Transformer variants,
          model compression, and on-device inference to make deployment on resource-constrained devices more 
          feasible [5].
        </p>
        <p>
          These challenges have motivated extensive work on optimizing Transformer models to reduce model size and 
          inference latency. Over the past 3 - 5 years, many techniques have been proposed and adopted, but most notably,
          quantization has emerged as one of the most practical and widely used approaches for compressing models and 
          accelerating inference, especially on hardware with efficient low-precision support. The core idea of 
          quantization is to represent model parameters (and sometimes activations) with lower numerical precision. 
          Instead of storing all weights in 32-bit floating-point format, recent methods use 8-bit and even 4-bit
          representations. In the ideal case, moving from 32-bit to 8-bit parameters reduces parameter storage by 75%,
          and moving to 4-bit reduces it by 87.5%, assuming all eligible weights are quantized. This substantially 
          lowers the memory footprint and, when low-precision operations are well supported by the hardware, can also 
          significantly reduce inference time.
        </p>

        <p>
          This article investigates an alternative axis of model compression that targets the attention mechanism itself,
          by factorizing the attention projection matrices into a product of two low-rank matrices. Most prior work on
          compressing Transformers, including pruning and the quantization techniques discussed above, has focused
          primarily on reducing the size of the Feed Forward Networks (FFNs) which is often the largest component of 
          modern day transformers. In contrast, applying aggressive pruning or low-precision quantization to multi-head 
          attention (MHA) can directly affect the model's outputs [6, 7]. In practice, attention projections are often more 
          sensitive to changes in structure or numerical precision, so naively pruning or quantizing these components 
          can lead to a significant drop in quality [7]. Our proposed FAP method reduces the number of trainable parameters
          in the attention layers by replacing full-rank projections with low-rank factorizations, with the goal of 
          reducing parameters while maintaining model quality in our experiments.
        </p>
      </section>

      <!-- Background -->
      <section id="Background">
        <h2> 2. Background </h2>

        <h3>2.1 Quanitzation</h3>
        <p>
          Quantization has become one of the most widely used methods for model compression because 
          it can substantially reduce memory and compute while often preserving model quality. Many practical 
          schemes primarily target lowering the precision of model parameters (particularly with FFNs), with 
          activations initially kept in higher precision. Most recent works on quantization methods can be categorized
          into two groups: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ) [7]. In QAT,
          the model is trained or fine-tuned while using low-precision parameters (e.g. 8-bits floating point),
          so that the optimization can compensate for quantization error. LLM-QAT is a representative example 
          that quantizes weights, activations, and even the KV cache, and then performs data-free distillation
          from the original model to counteract model degradation [8]. However, QAT typically requires substantial 
          compute and memory, which makes it difficult to apply to very large models in practice.
        </p>

        <p>
          On the other hand, PTQ starts from a fully trained model and quantizes it after training without fine-tuning 
          at all. Typical PTQ pipelines gather a small calibration set, estimate statistics of weights and activations, 
          and then choose quantization scales and zero-points (e.g. channel-wise or group-wise) to minimize some error 
          metric. For LLMs, many PTQ methods focus on weight-only quantization to 8-bits, 4-bits, or even 3-bits and can 
          often be applied with no additional training. Examples include ZeroQuant, which introduces efficient PTQ for
          large Transformer models [9]. Additionally, GPTQ and other related methods use approximate second-order 
          information to achieve accurate 3-bit to 4-bit weight-only quantization [10]. SmoothQuant, which enables 8-bit
          weights and activations for many LLM architectures via an activation-smoothing transformation [11]. Compared
          to QAT, PTQ is often easier and cheaper to deploy and is therefore the most common choice in real systems, 
          although its accuracy can degrade at extremely low bit-widths or under strong hardware constraints. 
        </p>

        <h3>2.2 Low-Rank Approximation</h3>
        <p>
          Training pipelines of Modern LLMs include large-scale pretraining on general-domain data before undergoing 
          adaptation to a particular task. However, recent models such as GPT, Gemini, and Claude use very large 
          transformer architectures, often with tens or hundreds of billions of parameters, and even consumer or open 
          models such as Mistral and LLaMA can have tens of billions of parameters. Thus, full fine-tuning (i.e., 
          updating every single weight within the model) becomes extremely expensive and often impractical. 
        </p>

        <p>
          To address this issue, researchers at Microsoft introduced LoRA (Low-Rank Adaptation of Large Language Models) 
          [12]. LoRA adds a small trainable module that represents the weight update as the product of two low-rank
           matrices, while the original model weights are kept frozen. The key assumption is that, for large 
           overparameterized models, the weight update during fine-tuning lies in a much lower-dimensional subspace. 
           Thus, the actual gradient update becomes the following:
        </p>

        <div style="position: relative; margin: 1em 0; font-family: 'Times New Roman', Times, serif; font-size: 1.5em;">
          <p style="text-align: center; font-style: italic; margin: 0;">
            h = W<sub>0</sub> + &Delta;W
          </p>
          <span style="position: absolute; right: 0; top: 0; font-style: normal;">
            (1)
          </span>
        </div>

        <p>
          In particular, we set <i>&Delta;W</i> = <i>A</i><i>B</i>, where <i>A</i> &in;
          &#8477;<sup><i>d</i><sub><i>out</i></sub> &times; <i>r</i></sup> and <i>B</i> &in; &#8477;<sup><i>r</i>
          &times; <i>d</i><sub><i>in</i></sub></sup>. These LoRA layers are inserted across the model in every transformer 
          block to approximate the necessary gradient updates for fine-tuning without having to update all of the 
          original weights. The final updated matrix is <i>h</i> as per equation 1.
        </p>

        <p>
          With the introduction of LoRA, it has inspired several follow-up methods such as QLoRA, which combines 
          low-rank adaptation with low-precision quantization [13]. QLoRA further decreases the memory footprint and 
          inference time by quantizing the base model weights to 4-bit precision while keeping a small set of LoRA 
          adapter weights in higher precision (i.e. 16-bit floating point) for training. In this setup, the quantized 
          base model stays fixed, and only the higher-precision LoRA parameters are updated, allowing the fine-tuned 
          model to approach the performance of a standard 16-bit or 32-bit model while using much less memory during 
          both training and inference.
        </p>

      </section>

      <!-- Methods / Main Idea -->
      <section id="methods">
        <h2>3. Methodology </h2>

        <h3 id="setup">3.1 Setup</h3>
        <p>
          Describe the objects and notation you will use. Keep it concrete:
          “We consider a model $f_\\theta: \\mathbb{R}^d \\to \\mathbb{R}^k$ trained on data
          $(x_i, y_i)$ with loss $L(\\theta)$…” (this will render if MathJax is enabled).
        </p>

        <div class="callout">
          <strong>Tip for yourself:</strong> keep notation lightweight; blog readers tolerate less notation
          than formal papers.
        </div>

        <h3 id="core-idea">3.2 Core idea</h3>
        <p>
          Explain the central claim or construction in plain language first. Imagine explaining it to a
          smart colleague from an adjacent field. Only after that, add formulas or diagrams.
        </p>

        <p>
          Example (with math if MathJax is on):
          “We propose to view the training dynamics as gradient flow on a low-dimensional manifold
          of effective representations, rather than in the full parameter space:
          $\\dot{z}_t = -\\nabla_z \\mathcal{L}(z_t)$, where $z_t$ is the representation state.”
        </p>

        <h3 id="theory">3.3 Theoretical observations (optional)</h3>
        <p>
          Use this subsection for the more formal claims, lemmas, or intuitions. You can mix light
          formalism with explanations:
        </p>

        <blockquote>
          Under simplifying assumptions (e.g., linearized dynamics, isotropic noise), the representation
          trajectories follow approximately straight lines in the space of sufficient statistics.
        </blockquote>

        <pre>
// Small code example (optional)
def toy_update(z, grad, lr=0.1):
    # Simple representation update step
    return z - lr * grad
        </pre>
      </section>

      <!-- Experiments -->
      <section id="experiments">
        <h2>4. Experiments</h2>

        <h3 id="experimental-setup">4.1 Experimental setup</h3>
        <p>
          Describe the dataset, models, and training setup. Keep it short and link to a repo if you have one.
        </p>

        <ul>
          <li><strong>Datasets:</strong> e.g., CIFAR-10, synthetic Gaussian mixtures, toy regression.</li>
          <li><strong>Models:</strong> e.g., 3-layer MLP, small transformer, CNN baseline.</li>
          <li><strong>Training:</strong> optimizer, learning rate, number of steps, key hyperparameters.</li>
        </ul>

        <h3 id="results">4.2 Results</h3>

        <figure>
          <img src="images/example-plot.png" alt="Placeholder plot for experimental results" />
          <figcaption>
            Figure 1: Placeholder description of your main quantitative result. Replace with your own plot.
          </figcaption>
        </figure>

        <p>
          Interpret the figure in words. What’s the pattern? Which comparisons matter? What should
          the reader take away that they wouldn’t get from just glancing at the plot?
        </p>

        <h3 id="qualitative">4.3 Qualitative examples (optional)</h3>
        <p>
          If relevant, show a few qualitative examples (e.g., model behaviors, attention patterns,
          visualization of representations). Explain why each example is interesting, not just that it exists.
        </p>
      </section>

      <!-- Discussion -->
      <section id="discussion">
        <h2>5. Discussion</h2>

        <h3 id="implications">5.1 Implications</h3>
        <p>
          Summarize what your perspective or results imply for practice or theory. Think:
          “If this view is right, how should we train, evaluate, or think about models differently?”
        </p>

        <h3 id="limitations">5.2 Limitations</h3>
        <p>
          Be explicit about scope. Where does your story clearly not apply? Which assumptions are
          unrealistic? What would make you update or abandon this perspective?
        </p>

        <h3 id="future-work">5.3 Open questions / future work</h3>
        <p>
          End with 2–4 concrete directions a reader could explore:
        </p>
        <ul>
          <li>Extend the framework to larger or more realistic models.</li>
          <li>Test specific predictions on new benchmarks.</li>
          <li>Connect this view to other theories (e.g., NTK, grokking, scaling laws).</li>
        </ul>
      </section>

      <!-- Conclusion -->
      <section id="conclusion">
        <h2>6. Conclusion</h2>
        <p>
          Write a short, non-technical recap in 2–3 paragraphs. Reiterate the main question, your key
          ideas, and your most important takeaway. Assume the reader skimmed everything else.
        </p>
      </section>

      <!-- References -->
      <section id="references">
        <h2>References</h2>
        <p style="font-size: 14px; color: var(--text-muted);">
          Use whatever citation style you like; for a blog, hyperlinks are usually enough.
        </p>
        <ul>
          <li>[1] Author et al. <em>Title of a related paper</em>, Conference, Year. <a href="#">link</a></li>
          <li>[2] Another Author. <em>Another reference</em>. <a href="#">link</a></li>
          <li>[3] Blog post: <em>Good prior writeup on a similar topic</em>. <a href="#">link</a></li>
        </ul>
      </section>
    </article>

    <!-- Sidebar / TOC -->
    <aside>
      <nav class="toc">
        <div class="toc-title">On this page</div>
        <small>Jump to any section</small>
        <ul>
          <li><a href="#abstract">Abstract</a></li>
          <li><a href="#introduction">1. Introduction</a></li>
          <li><a href="#Related-Works">2. Related Works</a>
            <ul class="toc-sub">
              <li><a href="#Quantization">2.1 Quantization</a></li>
              <li><a href="#Low-Rank-Approximation">2.2 Low Rank Approximation (LoRA)</a></li>
            </ul>
          </li>
          <li><a href="#methods">3. Main Idea / Framework</a>
            <ul class="toc-sub">
              <li><a href="#setup">3.1 Setup</a></li>
              <li><a href="#core-idea">3.2 Core idea</a></li>
              <li><a href="#theory">3.3 Theory</a></li>
            </ul>
          </li>
          <li><a href="#experiments">4. Experiments</a>
            <ul class="toc-sub">
              <li><a href="#experimental-setup">4.1 Setup</a></li>
              <li><a href="#results">4.2 Results</a></li>
              <li><a href="#qualitative">4.3 Qualitative</a></li>
            </ul>
          </li>
          <li><a href="#discussion">5. Discussion</a></li>
          <li><a href="#conclusion">6. Conclusion</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </nav>
    </aside>
  </main>

  <footer>
    © 2025 Your Name · Built as a simple static HTML template.
  </footer>
</div>
</body>
</html>
